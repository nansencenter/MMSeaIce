{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/antonk/miniconda/envs/mm_seaice/lib/python3.12/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/data1/antonk/miniconda/envs/mm_seaice/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from mmcv import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "from functions import create_train_validation_and_test_scene_list, get_model, get_loss, load_model\n",
    "from loaders import get_variable_options, AI4ArcticChallengeTestDataset, AI4ArcticChallengeDataset\n",
    "\n",
    "torch.set_num_threads(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| args_config: 'configs/sir_is2/sir_is2_02.py'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options train_list and validate_list initialised\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet_regression_SIR_IS2(\n",
       "  (input_block): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (contract_blocks): ModuleList(\n",
       "    (0): ContractingBlock(\n",
       "      (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ContractingBlock(\n",
       "      (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ContractingBlock(\n",
       "      (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bridge): ContractingBlock(\n",
       "    (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (double_conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (expand_blocks): ModuleList(\n",
       "    (0-1): 2 x ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_sir_1000ms_reg): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_config = 'configs/sir_is2/sir_is2_02.py'\n",
    "ic(args_config)\n",
    "cfg = Config.fromfile(args_config)\n",
    "train_options = cfg.train_options\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "train_options = get_variable_options(train_options)\n",
    "create_train_validation_and_test_scene_list(train_options)\n",
    "device = 'cpu'\n",
    "net = get_model(train_options, device)\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['input_block.double_conv.0.weight', 'input_block.double_conv.1.weight', 'input_block.double_conv.1.bias', 'input_block.double_conv.1.running_mean', 'input_block.double_conv.1.running_var', 'input_block.double_conv.1.num_batches_tracked', 'input_block.double_conv.3.weight', 'input_block.double_conv.4.weight', 'input_block.double_conv.4.bias', 'input_block.double_conv.4.running_mean', 'input_block.double_conv.4.running_var', 'input_block.double_conv.4.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.0.weight', 'contract_blocks.0.double_conv.double_conv.1.weight', 'contract_blocks.0.double_conv.double_conv.1.bias', 'contract_blocks.0.double_conv.double_conv.1.running_mean', 'contract_blocks.0.double_conv.double_conv.1.running_var', 'contract_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.3.weight', 'contract_blocks.0.double_conv.double_conv.4.weight', 'contract_blocks.0.double_conv.double_conv.4.bias', 'contract_blocks.0.double_conv.double_conv.4.running_mean', 'contract_blocks.0.double_conv.double_conv.4.running_var', 'contract_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.0.weight', 'contract_blocks.1.double_conv.double_conv.1.weight', 'contract_blocks.1.double_conv.double_conv.1.bias', 'contract_blocks.1.double_conv.double_conv.1.running_mean', 'contract_blocks.1.double_conv.double_conv.1.running_var', 'contract_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.3.weight', 'contract_blocks.1.double_conv.double_conv.4.weight', 'contract_blocks.1.double_conv.double_conv.4.bias', 'contract_blocks.1.double_conv.double_conv.4.running_mean', 'contract_blocks.1.double_conv.double_conv.4.running_var', 'contract_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.0.weight', 'contract_blocks.2.double_conv.double_conv.1.weight', 'contract_blocks.2.double_conv.double_conv.1.bias', 'contract_blocks.2.double_conv.double_conv.1.running_mean', 'contract_blocks.2.double_conv.double_conv.1.running_var', 'contract_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.3.weight', 'contract_blocks.2.double_conv.double_conv.4.weight', 'contract_blocks.2.double_conv.double_conv.4.bias', 'contract_blocks.2.double_conv.double_conv.4.running_mean', 'contract_blocks.2.double_conv.double_conv.4.running_var', 'contract_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'bridge.double_conv.double_conv.0.weight', 'bridge.double_conv.double_conv.1.weight', 'bridge.double_conv.double_conv.1.bias', 'bridge.double_conv.double_conv.1.running_mean', 'bridge.double_conv.double_conv.1.running_var', 'bridge.double_conv.double_conv.1.num_batches_tracked', 'bridge.double_conv.double_conv.3.weight', 'bridge.double_conv.double_conv.4.weight', 'bridge.double_conv.double_conv.4.bias', 'bridge.double_conv.double_conv.4.running_mean', 'bridge.double_conv.double_conv.4.running_var', 'bridge.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.0.weight', 'expand_blocks.0.double_conv.double_conv.1.weight', 'expand_blocks.0.double_conv.double_conv.1.bias', 'expand_blocks.0.double_conv.double_conv.1.running_mean', 'expand_blocks.0.double_conv.double_conv.1.running_var', 'expand_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.3.weight', 'expand_blocks.0.double_conv.double_conv.4.weight', 'expand_blocks.0.double_conv.double_conv.4.bias', 'expand_blocks.0.double_conv.double_conv.4.running_mean', 'expand_blocks.0.double_conv.double_conv.4.running_var', 'expand_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.0.weight', 'expand_blocks.1.double_conv.double_conv.1.weight', 'expand_blocks.1.double_conv.double_conv.1.bias', 'expand_blocks.1.double_conv.double_conv.1.running_mean', 'expand_blocks.1.double_conv.double_conv.1.running_var', 'expand_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.3.weight', 'expand_blocks.1.double_conv.double_conv.4.weight', 'expand_blocks.1.double_conv.double_conv.4.bias', 'expand_blocks.1.double_conv.double_conv.4.running_mean', 'expand_blocks.1.double_conv.double_conv.4.running_var', 'expand_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.0.weight', 'expand_blocks.2.double_conv.double_conv.1.weight', 'expand_blocks.2.double_conv.double_conv.1.bias', 'expand_blocks.2.double_conv.double_conv.1.running_mean', 'expand_blocks.2.double_conv.double_conv.1.running_var', 'expand_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.3.weight', 'expand_blocks.2.double_conv.double_conv.4.weight', 'expand_blocks.2.double_conv.double_conv.4.bias', 'expand_blocks.2.double_conv.double_conv.4.running_mean', 'expand_blocks.2.double_conv.double_conv.4.running_var', 'expand_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.0.weight', 'expand_blocks.3.double_conv.double_conv.1.weight', 'expand_blocks.3.double_conv.double_conv.1.bias', 'expand_blocks.3.double_conv.double_conv.1.running_mean', 'expand_blocks.3.double_conv.double_conv.1.running_var', 'expand_blocks.3.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.3.weight', 'expand_blocks.3.double_conv.double_conv.4.weight', 'expand_blocks.3.double_conv.double_conv.4.bias', 'expand_blocks.3.double_conv.double_conv.4.running_mean', 'expand_blocks.3.double_conv.double_conv.4.running_var', 'expand_blocks.3.double_conv.double_conv.4.num_batches_tracked', 'output_sir_1000ms_reg.weight', 'output_sir_1000ms_reg.bias'])\n",
      "dict_keys(['input_block.double_conv.0.weight', 'input_block.double_conv.1.weight', 'input_block.double_conv.1.bias', 'input_block.double_conv.1.running_mean', 'input_block.double_conv.1.running_var', 'input_block.double_conv.1.num_batches_tracked', 'input_block.double_conv.3.weight', 'input_block.double_conv.4.weight', 'input_block.double_conv.4.bias', 'input_block.double_conv.4.running_mean', 'input_block.double_conv.4.running_var', 'input_block.double_conv.4.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.0.weight', 'contract_blocks.0.double_conv.double_conv.1.weight', 'contract_blocks.0.double_conv.double_conv.1.bias', 'contract_blocks.0.double_conv.double_conv.1.running_mean', 'contract_blocks.0.double_conv.double_conv.1.running_var', 'contract_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.3.weight', 'contract_blocks.0.double_conv.double_conv.4.weight', 'contract_blocks.0.double_conv.double_conv.4.bias', 'contract_blocks.0.double_conv.double_conv.4.running_mean', 'contract_blocks.0.double_conv.double_conv.4.running_var', 'contract_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.0.weight', 'contract_blocks.1.double_conv.double_conv.1.weight', 'contract_blocks.1.double_conv.double_conv.1.bias', 'contract_blocks.1.double_conv.double_conv.1.running_mean', 'contract_blocks.1.double_conv.double_conv.1.running_var', 'contract_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.3.weight', 'contract_blocks.1.double_conv.double_conv.4.weight', 'contract_blocks.1.double_conv.double_conv.4.bias', 'contract_blocks.1.double_conv.double_conv.4.running_mean', 'contract_blocks.1.double_conv.double_conv.4.running_var', 'contract_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.0.weight', 'contract_blocks.2.double_conv.double_conv.1.weight', 'contract_blocks.2.double_conv.double_conv.1.bias', 'contract_blocks.2.double_conv.double_conv.1.running_mean', 'contract_blocks.2.double_conv.double_conv.1.running_var', 'contract_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.3.weight', 'contract_blocks.2.double_conv.double_conv.4.weight', 'contract_blocks.2.double_conv.double_conv.4.bias', 'contract_blocks.2.double_conv.double_conv.4.running_mean', 'contract_blocks.2.double_conv.double_conv.4.running_var', 'contract_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'bridge.double_conv.double_conv.0.weight', 'bridge.double_conv.double_conv.1.weight', 'bridge.double_conv.double_conv.1.bias', 'bridge.double_conv.double_conv.1.running_mean', 'bridge.double_conv.double_conv.1.running_var', 'bridge.double_conv.double_conv.1.num_batches_tracked', 'bridge.double_conv.double_conv.3.weight', 'bridge.double_conv.double_conv.4.weight', 'bridge.double_conv.double_conv.4.bias', 'bridge.double_conv.double_conv.4.running_mean', 'bridge.double_conv.double_conv.4.running_var', 'bridge.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.0.weight', 'expand_blocks.0.double_conv.double_conv.1.weight', 'expand_blocks.0.double_conv.double_conv.1.bias', 'expand_blocks.0.double_conv.double_conv.1.running_mean', 'expand_blocks.0.double_conv.double_conv.1.running_var', 'expand_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.3.weight', 'expand_blocks.0.double_conv.double_conv.4.weight', 'expand_blocks.0.double_conv.double_conv.4.bias', 'expand_blocks.0.double_conv.double_conv.4.running_mean', 'expand_blocks.0.double_conv.double_conv.4.running_var', 'expand_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.0.weight', 'expand_blocks.1.double_conv.double_conv.1.weight', 'expand_blocks.1.double_conv.double_conv.1.bias', 'expand_blocks.1.double_conv.double_conv.1.running_mean', 'expand_blocks.1.double_conv.double_conv.1.running_var', 'expand_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.3.weight', 'expand_blocks.1.double_conv.double_conv.4.weight', 'expand_blocks.1.double_conv.double_conv.4.bias', 'expand_blocks.1.double_conv.double_conv.4.running_mean', 'expand_blocks.1.double_conv.double_conv.4.running_var', 'expand_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.0.weight', 'expand_blocks.2.double_conv.double_conv.1.weight', 'expand_blocks.2.double_conv.double_conv.1.bias', 'expand_blocks.2.double_conv.double_conv.1.running_mean', 'expand_blocks.2.double_conv.double_conv.1.running_var', 'expand_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.3.weight', 'expand_blocks.2.double_conv.double_conv.4.weight', 'expand_blocks.2.double_conv.double_conv.4.bias', 'expand_blocks.2.double_conv.double_conv.4.running_mean', 'expand_blocks.2.double_conv.double_conv.4.running_var', 'expand_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.0.weight', 'expand_blocks.3.double_conv.double_conv.1.weight', 'expand_blocks.3.double_conv.double_conv.1.bias', 'expand_blocks.3.double_conv.double_conv.1.running_mean', 'expand_blocks.3.double_conv.double_conv.1.running_var', 'expand_blocks.3.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.3.weight', 'expand_blocks.3.double_conv.double_conv.4.weight', 'expand_blocks.3.double_conv.double_conv.4.bias', 'expand_blocks.3.double_conv.double_conv.4.running_mean', 'expand_blocks.3.double_conv.double_conv.4.running_var', 'expand_blocks.3.double_conv.double_conv.4.num_batches_tracked', 'output_sir_1000ms_reg.weight', 'output_sir_1000ms_reg.bias'])\n",
      "dict_keys(['input_block.double_conv.0.weight', 'input_block.double_conv.1.weight', 'input_block.double_conv.1.bias', 'input_block.double_conv.1.running_mean', 'input_block.double_conv.1.running_var', 'input_block.double_conv.1.num_batches_tracked', 'input_block.double_conv.3.weight', 'input_block.double_conv.4.weight', 'input_block.double_conv.4.bias', 'input_block.double_conv.4.running_mean', 'input_block.double_conv.4.running_var', 'input_block.double_conv.4.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.0.weight', 'contract_blocks.0.double_conv.double_conv.1.weight', 'contract_blocks.0.double_conv.double_conv.1.bias', 'contract_blocks.0.double_conv.double_conv.1.running_mean', 'contract_blocks.0.double_conv.double_conv.1.running_var', 'contract_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.3.weight', 'contract_blocks.0.double_conv.double_conv.4.weight', 'contract_blocks.0.double_conv.double_conv.4.bias', 'contract_blocks.0.double_conv.double_conv.4.running_mean', 'contract_blocks.0.double_conv.double_conv.4.running_var', 'contract_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.0.weight', 'contract_blocks.1.double_conv.double_conv.1.weight', 'contract_blocks.1.double_conv.double_conv.1.bias', 'contract_blocks.1.double_conv.double_conv.1.running_mean', 'contract_blocks.1.double_conv.double_conv.1.running_var', 'contract_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.3.weight', 'contract_blocks.1.double_conv.double_conv.4.weight', 'contract_blocks.1.double_conv.double_conv.4.bias', 'contract_blocks.1.double_conv.double_conv.4.running_mean', 'contract_blocks.1.double_conv.double_conv.4.running_var', 'contract_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.0.weight', 'contract_blocks.2.double_conv.double_conv.1.weight', 'contract_blocks.2.double_conv.double_conv.1.bias', 'contract_blocks.2.double_conv.double_conv.1.running_mean', 'contract_blocks.2.double_conv.double_conv.1.running_var', 'contract_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.3.weight', 'contract_blocks.2.double_conv.double_conv.4.weight', 'contract_blocks.2.double_conv.double_conv.4.bias', 'contract_blocks.2.double_conv.double_conv.4.running_mean', 'contract_blocks.2.double_conv.double_conv.4.running_var', 'contract_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'bridge.double_conv.double_conv.0.weight', 'bridge.double_conv.double_conv.1.weight', 'bridge.double_conv.double_conv.1.bias', 'bridge.double_conv.double_conv.1.running_mean', 'bridge.double_conv.double_conv.1.running_var', 'bridge.double_conv.double_conv.1.num_batches_tracked', 'bridge.double_conv.double_conv.3.weight', 'bridge.double_conv.double_conv.4.weight', 'bridge.double_conv.double_conv.4.bias', 'bridge.double_conv.double_conv.4.running_mean', 'bridge.double_conv.double_conv.4.running_var', 'bridge.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.0.weight', 'expand_blocks.0.double_conv.double_conv.1.weight', 'expand_blocks.0.double_conv.double_conv.1.bias', 'expand_blocks.0.double_conv.double_conv.1.running_mean', 'expand_blocks.0.double_conv.double_conv.1.running_var', 'expand_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.3.weight', 'expand_blocks.0.double_conv.double_conv.4.weight', 'expand_blocks.0.double_conv.double_conv.4.bias', 'expand_blocks.0.double_conv.double_conv.4.running_mean', 'expand_blocks.0.double_conv.double_conv.4.running_var', 'expand_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.0.weight', 'expand_blocks.1.double_conv.double_conv.1.weight', 'expand_blocks.1.double_conv.double_conv.1.bias', 'expand_blocks.1.double_conv.double_conv.1.running_mean', 'expand_blocks.1.double_conv.double_conv.1.running_var', 'expand_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.3.weight', 'expand_blocks.1.double_conv.double_conv.4.weight', 'expand_blocks.1.double_conv.double_conv.4.bias', 'expand_blocks.1.double_conv.double_conv.4.running_mean', 'expand_blocks.1.double_conv.double_conv.4.running_var', 'expand_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.0.weight', 'expand_blocks.2.double_conv.double_conv.1.weight', 'expand_blocks.2.double_conv.double_conv.1.bias', 'expand_blocks.2.double_conv.double_conv.1.running_mean', 'expand_blocks.2.double_conv.double_conv.1.running_var', 'expand_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.3.weight', 'expand_blocks.2.double_conv.double_conv.4.weight', 'expand_blocks.2.double_conv.double_conv.4.bias', 'expand_blocks.2.double_conv.double_conv.4.running_mean', 'expand_blocks.2.double_conv.double_conv.4.running_var', 'expand_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.0.weight', 'expand_blocks.3.double_conv.double_conv.1.weight', 'expand_blocks.3.double_conv.double_conv.1.bias', 'expand_blocks.3.double_conv.double_conv.1.running_mean', 'expand_blocks.3.double_conv.double_conv.1.running_var', 'expand_blocks.3.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.3.weight', 'expand_blocks.3.double_conv.double_conv.4.weight', 'expand_blocks.3.double_conv.double_conv.4.bias', 'expand_blocks.3.double_conv.double_conv.4.running_mean', 'expand_blocks.3.double_conv.double_conv.4.running_var', 'expand_blocks.3.double_conv.double_conv.4.num_batches_tracked', 'output_sir_1000ms_reg.weight', 'output_sir_1000ms_reg.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(net, 'sir_is2_01/best_model_sir_is2_01.pth', device=device, name_mapping={'sir_1000ms_seg': 'sir_1000ms_reg'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AI4ArcticChallengeDataset(files=train_options['train_list'][:3], options=train_options, do_transform=True)\n",
    "x, y = dataset[0]\n",
    "for name in y:\n",
    "    hh = np.ma.masked_where(x[0][0] == 255, x[0][0])\n",
    "    sir1 = np.ma.masked_where(y[name][0] == 255, y[name][0])\n",
    "    print(name)\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    imsh0 = axs[0].imshow(hh, interpolation='nearest')\n",
    "    imsh1 = axs[1].imshow(sir1, interpolation='nearest')\n",
    "    plt.colorbar(imsh0, ax=axs[0], shrink=0.5)\n",
    "    plt.colorbar(imsh1, ax=axs[1], shrink=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['validate_list'][:3], mode='train')\n",
    "x, y, cfv_masks, tfv_mask, name, original_size = dataset_val[0]\n",
    "print(x.shape, y['sir_1000ms_reg'].shape, cfv_masks['sir_1000ms_reg'].shape, tfv_mask.shape, name, original_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=None, shuffle=True, num_workers=train_options['num_workers'], pin_memory=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n",
    "loss_ce_functions = {chart: get_loss(train_options['chart_loss'][chart]['type'], chart=chart, **train_options['chart_loss'][chart])\n",
    "                         for chart in train_options['charts']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops though batches in queue.\n",
    "for i, (batch_x, batch_y) in enumerate(tqdm(iterable=dataloader_train, total=train_options['epoch_len'],\n",
    "                                            colour='red')):\n",
    "    # torch.cuda.empty_cache()  # Empties the GPU cache freeing up memory.\n",
    "    train_loss_batch = torch.tensor([0.]).to(device)  # Reset from previous batch.\n",
    "    #edge_consistency_loss = torch.tensor([0.]).to(device)\n",
    "    cross_entropy_loss = torch.tensor([0.]).to(device)\n",
    "    # - Transfer to device.\n",
    "    batch_x = batch_x.to(device, non_blocking=True)\n",
    "\n",
    "    # - Forward pass.\n",
    "    output = net(batch_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chart, weight in zip(train_options['charts'], train_options['task_weights']):\n",
    "\n",
    "    #if train_options['edge_consistency_loss'] != 0:\n",
    "    #    edge_consistency_loss = loss_water_edge_consistency(output)\n",
    "\n",
    "    if weight != 0:\n",
    "        _loss = loss_ce_functions[chart](output[chart], batch_y[chart].to(device))\n",
    "        print(chart, _loss)\n",
    "        #if torch.isnan(_loss):\n",
    "        #    import ipdb; ipdb.set_trace()\n",
    "        #cross_entropy_loss += weight * _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inf_y[chart])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce_functions[chart](\n",
    "                    output[chart],\n",
    "                    inf_y[chart].unsqueeze(0).long().to(device)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce_functions[chart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import class_decider, compute_metrics\n",
    "val_loss_sum = torch.tensor([0.])  # To sum the validation batch losses during the epoch.\n",
    "# To sum the validation cross entropy batch losses during the epoch.\n",
    "val_cross_entropy_loss_sum = torch.tensor([0.])\n",
    "\n",
    "outputs_flat = {chart: torch.Tensor().to(device) for chart in train_options['charts']}\n",
    "inf_ys_flat = {chart: torch.Tensor().to(device) for chart in train_options['charts']}\n",
    "# Outputs mask by train fill values\n",
    "outputs_tfv_mask = {chart: torch.Tensor().to(device) for chart in train_options['charts']}\n",
    "net.eval()  # Set network to evaluation mode.\n",
    "print('Validating...')\n",
    "# - Loops though scenes in queue.\n",
    "for i, (inf_x, inf_y, cfv_masks, tfv_mask, name, original_size) in enumerate(tqdm(iterable=dataloader_val,\n",
    "                                                                    total=len(train_options['validate_list']),\n",
    "                                                                    colour='green')):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Reset from previous batch.\n",
    "    # train fill value mask\n",
    "    # tfv_mask = (inf_x.squeeze()[0, :, :] == train_options['train_fill_value']).squeeze()\n",
    "    val_loss_batch = torch.tensor([0.]).to(device)\n",
    "    #val_edge_consistency_loss = torch.tensor([0.]).to(device)\n",
    "    val_cross_entropy_loss = torch.tensor([0.]).to(device)\n",
    "    # - Ensures that no gradients are calculated, which otherwise take up a lot of space on the GPU.\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda'):\n",
    "        inf_x = inf_x.to(device, non_blocking=True)\n",
    "        if train_options['model_selection'] == 'swin':\n",
    "            output = slide_inference(inf_x, net, train_options, 'val')\n",
    "            # output = batched_slide_inference(inf_x, net, train_options, 'val')\n",
    "        else:\n",
    "            output = net(inf_x)\n",
    "\n",
    "        for chart, weight in zip(train_options['charts'], train_options['task_weights']):\n",
    "            # SIR\n",
    "            if 'SIR' in train_options['charts'][0]:\n",
    "                val_cross_entropy_loss += weight * loss_ce_functions[chart](\n",
    "                    output[chart][:,:,~cfv_masks[chart]],\n",
    "                    inf_y[chart][~cfv_masks[chart]].unsqueeze(0).long().to(device)\n",
    "                )\n",
    "            else:\n",
    "                # SIC, SOD, FLZ\n",
    "                val_cross_entropy_loss += weight * loss_ce_functions[chart](\n",
    "                    output[chart],\n",
    "                    inf_y[chart].unsqueeze(0).long().to(device)\n",
    "                )\n",
    "\n",
    "        #if train_options['edge_consistency_loss'] != 0:\n",
    "        #    a = train_options['edge_consistency_loss']\n",
    "        #    val_edge_consistency_loss = a*loss_water_edge_consistency(output)\n",
    "\n",
    "    val_loss_batch = val_cross_entropy_loss #+ val_edge_consistency_loss\n",
    "    #raise\n",
    "    # - Final output layer, and storing of non masked pixels.\n",
    "    for chart in train_options['charts']:\n",
    "        # TODO:\n",
    "        # use class decider of output is classification, not regression\n",
    "        #output[chart] = class_decider(output[chart], train_options, chart)\n",
    "        outputs_flat[chart] = torch.cat((outputs_flat[chart], output[chart].squeeze()[~cfv_masks[chart].squeeze()]))\n",
    "        inf_ys_flat[chart] = torch.cat((inf_ys_flat[chart], inf_y[chart][~cfv_masks[chart]].to(device, non_blocking=True)))\n",
    "    # - Add batch loss.\n",
    "    val_loss_sum += val_loss_batch.detach().item()\n",
    "    val_cross_entropy_loss_sum += val_cross_entropy_loss.detach().item()\n",
    "\n",
    "# - Average loss for displaying\n",
    "val_loss_epoch = torch.true_divide(val_loss_sum, i + 1).detach().item()\n",
    "val_cross_entropy_epoch = torch.true_divide(val_cross_entropy_loss_sum, i + 1).detach().item()\n",
    "#val_edge_consistency_epoch = torch.true_divide(val_edge_consistency_loss_sum, i + 1).detach().item()\n",
    "\n",
    "# - Compute the relevant scores.\n",
    "print('Computing Metrics on Val dataset')\n",
    "combined_score, scores = compute_metrics(true=inf_ys_flat, pred=outputs_flat, charts=train_options['charts'],\n",
    "                                            metrics=train_options['chart_metric'], num_classes=train_options['n_classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(inf_ys_flat['sir_1000ms_reg'], outputs_flat['sir_1000ms_reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_config = 'configs/sir/sir_01.py'\n",
    "ic(args_config)\n",
    "cfg = Config.fromfile(args_config)\n",
    "train_options = cfg.train_options\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "train_options = get_variable_options(train_options)\n",
    "create_train_validation_and_test_scene_list(train_options)\n",
    "device = 'cpu'\n",
    "net = get_model(train_options, device)\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AI4ArcticChallengeDataset(files=train_options['train_list'][:5], options=train_options, do_transform=True)\n",
    "x, y = dataset[0]\n",
    "for name in y:\n",
    "    sir1 = np.ma.masked_where(y[name][0] == 255, y[name][0])\n",
    "    print(name)\n",
    "    plt.imshow(sir1, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['validate_list'][:3], mode='train')\n",
    "x, y, cfv_masks, tfv_mask, name, original_size = dataset_val[0]\n",
    "print(x.shape, y['sir_eqs0250'].shape, cfv_masks['sir_eqs0250'].shape, tfv_mask.shape, name, original_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sod = np.ma.masked_where(y['sir_eqs0250'] == 255, y['sir_eqs0250'])\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "imsh2 = axs[2].imshow(sod, interpolation='nearest')\n",
    "plt.colorbar(imsh2, shrink=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=None, shuffle=True, num_workers=train_options['num_workers'], pin_memory=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n",
    "loss_ce_functions = {chart: get_loss(train_options['chart_loss'][chart]['type'], chart=chart, **train_options['chart_loss'][chart])\n",
    "                         for chart in train_options['charts']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inf_x, inf_y, cfv_masks, tfv_mask, name, original_size) in enumerate(tqdm(iterable=dataloader_val,\n",
    "                                                                    total=len(train_options['validate_list']),\n",
    "                                                                    colour='green')):\n",
    "    with torch.no_grad():\n",
    "        inf_x = inf_x.to(device, non_blocking=True)\n",
    "        output = net(inf_x)\n",
    "\n",
    "        for chart, weight in zip(train_options['charts'], train_options['task_weights']):\n",
    "\n",
    "            val_cross_entropy_loss = weight * loss_ce_functions[chart](\n",
    "                output[chart][:,:,~cfv_masks[chart]],\n",
    "                inf_y[chart][~cfv_masks[chart]].unsqueeze(0).long().to(device)\n",
    "            )\n",
    "            print(name, chart, val_cross_entropy_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops though batches in queue.\n",
    "for i, (batch_x, batch_y) in enumerate(tqdm(iterable=dataloader_train, total=train_options['epoch_len'],\n",
    "                                            colour='red')):\n",
    "    # torch.cuda.empty_cache()  # Empties the GPU cache freeing up memory.\n",
    "    train_loss_batch = torch.tensor([0.]).to(device)  # Reset from previous batch.\n",
    "    #edge_consistency_loss = torch.tensor([0.]).to(device)\n",
    "    cross_entropy_loss = torch.tensor([0.]).to(device)\n",
    "    # - Transfer to device.\n",
    "    batch_x = batch_x.to(device, non_blocking=True)\n",
    "\n",
    "    # - Forward pass.\n",
    "    output = net(batch_x)\n",
    "    # breakpoint()\n",
    "    # - Calculate loss.\n",
    "    for chart, weight in zip(train_options['charts'], train_options['task_weights']):\n",
    "\n",
    "        #if train_options['edge_consistency_loss'] != 0:\n",
    "        #    edge_consistency_loss = loss_water_edge_consistency(output)\n",
    "\n",
    "        if weight != 0:\n",
    "            _loss = loss_ce_functions[chart](output[chart], batch_y[chart].to(device))\n",
    "            print(chart, _loss)\n",
    "            #if torch.isnan(_loss):\n",
    "            #    import ipdb; ipdb.set_trace()\n",
    "            cross_entropy_loss += weight * _loss\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_config = 'configs/sic_mse/sic_mse_maud_01g.py'\n",
    "ic(args_config)\n",
    "cfg = Config.fromfile(args_config)\n",
    "train_options = cfg.train_options\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "train_options = get_variable_options(train_options)\n",
    "device = 'cpu'\n",
    "net = get_model(train_options, device)\n",
    "create_train_validation_and_test_scene_list(train_options)\n",
    "dataset = AI4ArcticChallengeDataset(files=train_options['train_list'][:5], options=train_options, do_transform=True)\n",
    "\n",
    "x, y = dataset[0]\n",
    "sod = np.ma.masked_where(y['SOD'][0] == 255, y['SOD'][0])\n",
    "sic = np.ma.masked_where(y['SIC'][0] == 255, y['SIC'][0])\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "imsh2 = axs[2].imshow(sod)\n",
    "plt.colorbar(imsh2, ax=axs[2], shrink=0.5)\n",
    "imsh3 = axs[3].imshow(sic)\n",
    "plt.colorbar(imsh3, ax=axs[3], shrink=0.5)\n",
    "plt.show()\n",
    "\n",
    "dataset_val = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['validate_list'][:3], mode='train')\n",
    "x, y, cfv_masks, tfv_mask, name, original_size = dataset_val[0]\n",
    "print(x.shape, y['SOD'].shape, cfv_masks['SOD'].shape, tfv_mask.shape, name, original_size)\n",
    "sod = np.ma.masked_where(y['SOD'] == 255, y['SOD'])\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "imsh2 = axs[2].imshow(sod)\n",
    "plt.colorbar(imsh2, shrink=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0]\n",
    "sod = np.ma.masked_where(y['SOD'][0] == 255, y['SOD'][0])\n",
    "sic = np.ma.masked_where(y['SIC'][0] == 255, y['SIC'][0])\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "imsh2 = axs[2].imshow(sod)\n",
    "plt.colorbar(imsh2, ax=axs[2], shrink=0.2)\n",
    "imsh3 = axs[3].imshow(sic)\n",
    "plt.colorbar(imsh3, ax=axs[3], shrink=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, cfv_masks, tfv_mask, name, original_size in dataset_val:\n",
    "    print(x.shape, y['SOD'].shape, cfv_masks['SOD'].shape, tfv_mask.shape, name, original_size)\n",
    "    sod = np.ma.masked_where(cfv_masks['SOD'], y['SOD'])\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    axs[0].imshow(x[0, 0])\n",
    "    axs[1].imshow(x[0, 5])\n",
    "    imsh2 = axs[2].imshow(sod)\n",
    "    plt.colorbar(imsh2, shrink=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_config = 'configs/sic_mse/sic_mse_maud_02c.py'\n",
    "ic(args_config)\n",
    "cfg = Config.fromfile(args_config)\n",
    "train_options = cfg.train_options\n",
    "# Get options for variables, amsrenv grid, cropping and upsampling.\n",
    "train_options = get_variable_options(train_options)\n",
    "device = 'cpu'\n",
    "net = get_model(train_options, device)\n",
    "create_train_validation_and_test_scene_list(train_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AI4ArcticChallengeDataset(files=train_options['train_list'][:5], options=train_options, do_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0]\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "axs[2].imshow(y['SIR02'][0], clim=[0, 7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=None, shuffle=True, num_workers=train_options['num_workers'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataloader_train:\n",
    "    break\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "axs[2].imshow(y['SIR06'][0], clim=[0, 7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = AI4ArcticChallengeTestDataset(options=train_options, files=train_options['validate_list'], mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, cfv_masks, tfv_mask, name, original_size = dataset_val[4]\n",
    "print(x.shape, y['SIR00'].shape, cfv_masks['SIR00'].shape, tfv_mask.shape, name, original_size)\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(x[0, 0])\n",
    "axs[1].imshow(x[0, 5])\n",
    "axs[2].imshow(y['SIR00'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=None, num_workers=train_options['num_workers_val'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce_functions = {chart: get_loss(train_options['chart_loss'][chart]['type'], chart=chart, **train_options['chart_loss'][chart])\n",
    "                         for chart in train_options['charts']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inf_x, inf_y, cfv_masks, tfv_mask, name, original_size) in enumerate(tqdm(iterable=dataloader_val,\n",
    "                                                                    total=len(train_options['validate_list']),\n",
    "                                                                    colour='green')):\n",
    "    with torch.no_grad():\n",
    "        inf_x = inf_x.to(device, non_blocking=True)\n",
    "        output = net(inf_x)\n",
    "\n",
    "        for chart, weight in zip(train_options['charts'], train_options['task_weights']):\n",
    "\n",
    "            val_cross_entropy_loss = weight * loss_ce_functions[chart](\n",
    "                output[chart][:,:,~cfv_masks[chart]],\n",
    "                inf_y[chart][~cfv_masks[chart]].unsqueeze(0).long().to(device)\n",
    "            )\n",
    "    print(name, val_cross_entropy_loss, np.unique(inf_y['SIR12']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[chart].shape, inf_y[chart].shape, inf_y[chart].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce_functions[chart](output[chart], inf_y[chart].unsqueeze(0).long().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output = output[chart][:,:,~cfv_masks[chart]]\n",
    "_target = inf_y[chart][~cfv_masks[chart]].unsqueeze(0).long()\n",
    "_output.shape, _target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce_functions[chart](_output, _target.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(loss_ce_functions[chart])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cfv_masks['SIR12'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm_seaice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
